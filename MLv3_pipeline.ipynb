{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PatwaV01\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\PatwaV01\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler,QuantileTransformer\n",
    "from sklearn.cross_validation import StratifiedKFold,KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV,cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.feature_selection import SelectFromModel,SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation, feature_selection, pipeline,preprocessing, linear_model, grid_search\n",
    "from sklearn import decomposition\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install imbalanced learn using below on command prompt ( with Anaconda installed):\n",
    "    conda install -c glemaitre imbalanced-learn\n",
    "    Alternatively, follow the steps here:\n",
    "    http://pandas-ml.readthedocs.io/en/latest/imbalance.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Read the data starting with row 2 (based on obervation in bash prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #Function to read data from row 2 as header and onwards\n",
    "    data = pd.read_csv('LendingClub2012to2013.csv',header=1,low_memory=False)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_loan_status(x):\n",
    "    #Function To Transofrm loan status to binary classification problem\n",
    "    status = 0\n",
    "    if x in ['Fully Paid','In Grace Period']:\n",
    "        #Current status does not provide any useful information\n",
    "        # We will still consider as acceptable status as\n",
    "        status = 0\n",
    "        return status # Indicating Acceptable status\n",
    "    elif x in ['Late (16-30 days)','Default','Late (31-120 days)','Charged Off']:\n",
    "        status = 1\n",
    "        return status\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_redundancy(model_categorical,model_continuous):\n",
    "    status =0\n",
    "    for x in model_continuous:\n",
    "        if x in model_categorical:\n",
    "            status =1\n",
    "            print(x,' is in model_categorical too')\n",
    "    for x in model_categorical:\n",
    "        if x in model_continuous:\n",
    "            status =1\n",
    "            print(x,' is in model_continuous too')\n",
    "    if status == 0:\n",
    "        print('No Duplicate features')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(df):\n",
    "    df['int_rate'] = df['int_rate'].apply(lambda x: x.replace('%',''))\n",
    "    df['int_rate'] = df['int_rate'].apply(pd.to_numeric)\n",
    "    df['revol_util'] =df['revol_util'].astype(str).apply(lambda x: x.replace('%','')).replace('nan','')\n",
    "    df['revol_util'] =df['revol_util'].apply(pd.to_numeric)\n",
    "    df['emp_length'] = df['emp_length'].apply(lambda x: x.split()[0].replace('+','').replace('<','').replace('<','').replace('n/a',''))\n",
    "    df['emp_length'] =df['emp_length'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_useful_columns(data,model_continuous,model_categorical):\n",
    "    data.application_type.value_counts() # dropping the column\n",
    "    data.emp_title.nunique() # Too many unique values dropping the column\n",
    "    data.initial_list_status.value_counts() \n",
    "    data.pymnt_plan.value_counts()  # dropping the column\n",
    "    print('application_type : dropping the column: No useful information')\n",
    "    print('pymnt_plan : dropping the column: No useful information')\n",
    "    print('emp_title : dropping the column: Too many unique values dropping the column')\n",
    "    model_categorical.remove('application_type')\n",
    "    model_categorical.remove('pymnt_plan')\n",
    "    model_categorical.remove('emp_title')\n",
    "\n",
    "    #Unlist the features with more than 50% mising values from feature selection\n",
    "    print('Unlisted Below features with more than 50% mising values from feature selection')\n",
    "    for x in data[model_continuous].columns:\n",
    "        if (data[x].count()/len(data)) <0.5:\n",
    "            print(x)\n",
    "            model_continuous.remove(x)\n",
    "    for x in data[model_categorical].columns:\n",
    "        if (data[x].count()/len(data)) <0.5:\n",
    "            print(x)\n",
    "            model_categorical.remove(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_leakage():\n",
    "    status =0\n",
    "    leakage = ['int_rate','issue_d', 'recoveries', 'collection_recovery_fee', 'last_fico_range_high', 'last_fico_range_low', 'last_credit_pull_d', 'total_rec_prncp', 'last_pymnt_amnt', 'total_pymnt', 'total_pymnt_inv', 'last_pymnt_d', 'total_rec_late_fee', 'total_rec_int', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'out_prncp', 'out_prncp_inv', 'pymnt_plan', 'next_pymnt_d']\n",
    "    for x in hybrid:\n",
    "        if x in leakage:\n",
    "            print('Leakage column selected! : ',x)\n",
    "            status =1\n",
    "    if status == 0:\n",
    "        print('No leakage columns in selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(data):\n",
    "    data_imputed = data[hybrid]\n",
    "    data_imputed[model_categorical] = data[model_categorical].apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "    data_imputed[model_continuous] = data[model_continuous].apply(lambda x:x.fillna(x.mean()))\n",
    "    if max(pd.isnull(data_imputed[hybrid]).any(axis=1)) == False:\n",
    "        print('Data Imputedation complete. There are no missing values in returned dataframe')\n",
    "    return data_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_code_and_drop_correlated_features(data_imputed,model_continuous,model_categorical):\n",
    "    #Convert Categorical Variables into dummy variables and drop highly correlated variables\n",
    "    data_with_dummies = pd.get_dummies(data_imputed,columns=model_categorical)\n",
    "    cor = data_with_dummies.corr()\n",
    "    cor.loc[:,:] = np.tril(cor, k=-1) \n",
    "    cor = cor.stack()\n",
    "    print('Below are the correlated features:')\n",
    "    print(cor[(cor > 0.645) | (cor < -0.645)])\n",
    "    dropped_continuous_correlated = ['funded_amnt','funded_amnt_inv','installment','num_sats','initial_list_status_w','term_ 60 months'] \n",
    "    \n",
    "    model_continuous.remove('funded_amnt')\n",
    "    model_continuous.remove('funded_amnt_inv')\n",
    "    model_continuous.remove('installment')\n",
    "    model_continuous.remove('num_sats')\n",
    "#     model_continuous.remove('initial_list_status_w')\n",
    "  #  model_continuous.remove('term_ 60 months')\n",
    "    model_continuous =  [x for x in model_continuous if x not in dropped_continuous_correlated]\n",
    "    #Drop the Highly correlated variables\n",
    "    print('Dropped Highly correlated redundant features: ',dropped_continuous_correlated)\n",
    "    data_with_dummies.drop(dropped_continuous_correlated,inplace=True,axis=1)\n",
    "    return data_with_dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train,X_test,model_continuous):\n",
    "    scaler = StandardScaler()\n",
    "    X_train[model_continuous] =scaler.fit_transform(X_train[model_continuous])\n",
    "    X_test[model_continuous] =scaler.transform(X_test[model_continuous])\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_and_smote(data_with_dummies):\n",
    "    y = data_with_dummies['loan_status']\n",
    "    X = data_with_dummies.drop('loan_status',axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    print(\"Before we proceed further, let's split' the 25% data as test set and use SMOTE to balance the dependent variable.\")\n",
    "    print('It Appears that Our dependent variable is imbalanced.We are using Imbalanced learn package to use SMOTE algorithm in order to handle oversampling for unacceptable loan status data points')\n",
    "    sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    y_train = pd.DataFrame(y_train_res,columns=['loan_status'])\n",
    "    X_train = pd.DataFrame(X_train_res, columns=X_train.columns)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    report = classification_report(y_test,y_pred)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorial_bivariate(data,model_categorical,only_bad_loans=0):\n",
    "    data_vis = data\n",
    "    if only_bad_loans == 1:\n",
    "        data_vis = data[data.loan_status == 1]\n",
    "        \n",
    "    for x in model_categorical:\n",
    "        if data_vis[x].nunique() >5:\n",
    "            plt.figure(figsize=(20,4), dpi=700)\n",
    "        sns.countplot(data_vis[x],hue=data_vis.loan_status)\n",
    "        plt.savefig('categorical\\\\' + x)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pca(X_train,X_test,components=5):\n",
    "    pca = decomposition.PCA(n_components=components)\n",
    "    X_train_full = X_train\n",
    "    X_test_full = X_test\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    return X_train_full,X_test_full,X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_continuous_univariate(data,model_continuous):\n",
    "    for x in model_continuous:\n",
    "        sns.distplot(data[x])\n",
    "        plt.savefig('continuous_univariate\\\\' + x)\n",
    "        plt.show()\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "data =read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring categories and counts in our dependent variable 'loan status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4), dpi=700)\n",
    "sns.countplot(x=\"loan_status\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Fully paid loans are highest in count - We can combine late payments and charged off/defaults into a single category for better analysis as it and to reduced the scewness in our dependent variable 'loan_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['loan_status']=data['loan_status'].apply(lambda x: transform_loan_status(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the missing values in Dependent Variable\n",
    "data.dropna(subset = ['loan_status'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(x='loan_status',data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Select Features - Initial Selection:\n",
    "These features are selected based on Business significance and not purely statistical one as this is considered a best practice : Whenever we have enough information about problem domain, it should be the first filter in feature selection unless we have a really good reason for not doing so.\n",
    "We are creating separate lists for categorical and continuous features as they wil be analyzed/processed Separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_continuous = list(set(['annual_inc','acc_now_delinq','annual_inc_joint','bc_util'\n",
    " ,'chargeoff_within_12_mths','collections_12_mths_ex_med','delinq_2yrs'\n",
    " ,'delinq_amnt','dti','dti_joint','emp_length','fico_range_low','funded_amnt'\n",
    " ,'funded_amnt_inv','installment','loan_amnt','mort_acc'\n",
    " ,'mths_since_last_delinq','mths_since_recent_bc_dlq','num_accts_ever_120_pd'\n",
    " ,'num_bc_sats','num_sats','num_tl_90g_dpd_24m','open_acc','pct_tl_nvr_dlq','percent_bc_gt_75'\n",
    " ,'pub_rec','pub_rec_bankruptcies','revol_util','avg_cur_bal','total_acc','total_cu_tl','inq_last_6mths']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_categorical = list(set(['addr_state','application_type','grade','emp_title',\n",
    "                   'home_ownership', 'initial_list_status','pymnt_plan','term', 'purpose',  'verification_status','verification_status_joint']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that features are not repeated in two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_redundancy(model_categorical,model_continuous)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_non_useful_columns(data,model_continuous,model_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sklearn.preprocessing package's \n",
    "imputer method cannot effective handle categorical data m we are using below trick \n",
    "to impute missing category with most frequent category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid = ['loan_status'] + model_continuous+model_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_leakage() # Check that there are no leakage features in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = impute_missing(data[hybrid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorial_bivariate(data_imputed,model_categorical,only_bad_loans=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous_univariate(data_imputed,model_continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Categorical Variables into dummy variables and drop highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_with_dummies =  dummy_code_and_drop_correlated_features(data_imputed,model_continuous,model_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed further, let's split' the 25% data as test set and use SMOTE to balance the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Appears that Our dependent variable is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Imbalanced learn package to use SMOTE algorithm in order to handle \n",
    "oversampling for unacceptable loan status data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep 25% data as Test data and perform oversampling to balance the loan_status using SMOTE\n",
    "X_train, X_test, y_train, y_test = split_test_and_smote(data_with_dummies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore K Best Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of K below to play around with different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = SelectKBest(k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xtrain = kbest.fit_transform(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = kbest.get_support()\n",
    "new_features = X_train.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_train['loan_status'],data = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use StandardScaler to Scale the data\n",
    "- This is needed as we are using classifiers which are sensitive to the scale of distance measure and it is necessary to have a uniform scale for all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = scale_data(X_train,X_test,model_continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Decision Tree to Explore feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopKDecisionTreeFeatures(X_train,y_train,k):\n",
    "        \n",
    "    dt = DecisionTreeClassifier()\n",
    "\n",
    "    dt.fit(X_train,y_train)\n",
    "\n",
    "    features = pd.DataFrame(data= list(dt.feature_importances_),index=list(X_train.columns),columns=['importance']).reset_index()\n",
    "\n",
    "    sorted_features = features.sort_values(by=['importance'],ascending=False)\n",
    "\n",
    "    return (list(sorted_features['index'])[0:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class TopKDecisionTreeFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,k=50):\n",
    "        self.k = k\n",
    "        self.dt = DecisionTreeClassifier()\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.dt.fit(X_train,y_train)\n",
    "        self.features = pd.DataFrame(data= list(self.dt.feature_importances_),\n",
    "                                     index=list(X_train.columns),columns=['importance']).reset_index()\n",
    "        self.sorted_features = self.features.sort_values(by=['importance'],ascending=False)\n",
    "        self.topk = list(self.sorted_features['index'])[0:self.k]\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        return(x[self.topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT K Best Features - find number K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the list of top  features ( we have selected this number based on putput of SelectKBest pipeline) - Warning - This takes hours to run - Please comment till PCA step to skip it  or Uncomment to run it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_pipeline = make_pipeline(TopKDecisionTreeFeatures(),DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_parameters = dict(topkdecisiontreefeatures__k=[40,75,90,103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = sklearn.model_selection.KFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = GridSearchCV(dt_pipeline, param_grid=dt_parameters, scoring='f1', cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best estimator:\n",
    "Pipeline(memory=None,\n",
    "     steps=[('topkdecisiontreefeatures', TopKDecisionTreeFeatures(k=40)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV  for RandomForestClassifier takes long time to find optimal model  (more than 8 hours for the 48 features. For this reason, we will use PCA to reduce the number of features to 10 while preserving most of the variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction using PCA\n",
    "## Below code can be commented to see the performance difference with and without PCA ( Also rerun whole Notebook from the beginning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full,X_test_full,X_train,X_test = process_pca(X_train,X_test,components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running classifiers using default parameters for Initial judgement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_report(nbc,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_report(lc,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_report(dtc,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is the most processor intensive algorithm out of teh nes we have picked, we will use dimensionality reducted using PCA with just 4 componets in order to increase execution time ( Otherwise it runs for manyhours without output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnc = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full,X_test_full,X_train,X_test = process_pca(X_train, X_test, components=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classification_report(knnc,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover Original dataset Before PCA - Please skip this to run further part in reasonable amount of time\n",
    "X_train,X_test = X_train_full,X_test_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Fold cross validation using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = sklearn.model_selection.KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to processing limitations, we are again using PCA with 15 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full,X_test_full,X_train,X_test = process_pca(X_train, X_test, components=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipeline = make_pipeline(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_parameters = dict(randomforestclassifier__n_estimators=[50,75,130])\n",
    "cv = sklearn.model_selection.KFold(n_splits=10)\n",
    "dt_parameters = dict(decisiontreeclassifier__criterion=['entropy','gini'],decisiontreeclassifier__max_depth=[None,10,20,40,60])\n",
    "dt = GridSearchCV(dt_pipeline, param_grid=dt_parameters,scoring='f1', cv=cv,n_jobs=-1)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dt.fit(X_train,y_train)\n",
    "dt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classification_report(dt,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I do not have good intuttion about bayesian statistics, I am unsure about providing set of priors for cross validation algorithm to experiment with. This is something not covered in class and not implemented below using GridSearchCV - as we are not checking any hyperparameters, we are using KFold cross validation only in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = cross_val_score(nbc,X_train,y_train,scoring='f1',cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classification_report(nbc,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover Original dataset Before PCA\n",
    "X_train,X_test = X_train_full,X_test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = make_pipeline(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_parameters = dict(logisticregression__penalty=['l2','l1'],logisticregression__C =[1.0,0.1,0.3,0.5,0.8,0.05,0.01,0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg = GridSearchCV(lr_pipeline, param_grid=lr_parameters, scoring='f1', cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore ,Best estimator for data with 15 Principal components:\n",
    "Pipeline(memory=None,\n",
    "     steps=[('logisticregression', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also  ,Best estimator for data with data without dimensionality reduction ( This ran for 8 hours 48 minutes):\n",
    "Pipeline(memory=None,\n",
    "     steps=[('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcfinal = LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcfinal = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_report(lrcfinal,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classification_report(lrg,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is the most processor intensive algorithm out of teh nes we have picked, we will use dimensionality reducted using PCA with just 4 componets in order to increase execution time ( Otherwise it runs for manyhours without output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full,X_test_full,X_train,X_test = process_pca(X_train, X_test, components=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = make_pipeline(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_parameters = dict(kneighborsclassifier__n_neighbors=[2,5,10,20,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = GridSearchCV(dt_pipeline, param_grid=dt_parameters,scoring='f1', cv=cv,n_jobs=-1)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "knn.fit(X_train,y_train)\n",
    "knn.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_report(knn,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal value of K for KNN classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = make_pipeline(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameters = dict(randomforestclassifier__n_estimators=[50,100,200,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfg = GridSearchCV(rf_pipeline, param_grid=rf_parameters, scoring='f1', cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classification_report(rfg,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_pipeline = make_pipeline(SelectKBest(),DecisionTreeClassifier(),GaussianNB())\n",
    "# dt_parameters = dict(selectkbest__k=[50,75,'all'],\n",
    "#                      randomforestclassifier__n_estimators=[50,75,130])\n",
    "# cv = sklearn.model_selection.KFold(n_splits=10)\n",
    "# dt = GridSearchCV(dt_pipeline, param_grid=dt_parameters, scoring='f1', cv=cv)\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# dt.fit(X_train,y_train)\n",
    "# dt.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prat =['loan_amnt', 'grade', 'emp_length', 'annual_inc', 'loan_status', 'dti',\n",
    "#        'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal',\n",
    "#        'revol_util', 'collections_12_mths_ex_med', 'acc_now_delinq',\n",
    "#        'chargeoff_within_12_mths', 'delinq_amnt', 'fico_average',\n",
    "#        'home_ownership_MORTGAGE', 'home_ownership_NONE',\n",
    "#        'home_ownership_OTHER', 'home_ownership_OWN',\n",
    "#        'verification_status_Not Verified',\n",
    "#        'verification_status_Source Verified', 'purpose_car',\n",
    "#        'purpose_credit_card', 'purpose_home_improvement', 'purpose_house',\n",
    "#        'purpose_major_purchase', 'purpose_medical', 'purpose_moving',\n",
    "#        'purpose_other', 'purpose_renewable_energy', 'purpose_small_business',\n",
    "#        'purpose_vacation', 'purpose_wedding', 'term_ 36 months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in hybrid:\n",
    "#     if x not in prat:\n",
    "#         print(x,' Not in Prat list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for x in prat:\n",
    "#     if x not in hybrid:\n",
    "#         print(x,' Not in hybrid list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_categorical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
